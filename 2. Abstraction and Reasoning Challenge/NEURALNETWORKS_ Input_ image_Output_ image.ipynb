{"cells":[{"metadata":{},"cell_type":"markdown","source":"Guys!\n\nThis notebook is to help but also to ask for help, because I made it work for the TRAINING images, but not for the PREDICTED image, which is the difficult part.\n\nThose of you who want some help, consider this:\n- I use a Neural Network of 4 layers to predict the values of each pixel of the output matrix. It works quite fine! It can be used to TRAIN ANY task but with the following constraint.\n- Constraint: it only works if the SHAPE of the train images are the same. In other words, the dimensions of the input matrices must be the same. Also the dimensions of the output matrices. If a task fulfill this, it works. Example: Input images: all of them (3x3) -->  Output images: all of them (9x9). This works fine!\n                                                        \n- PROBLEM: I'm not able to make the NeuralNetwork to learn patterns. Instead, it learns the weights to reach any output. This is the reason why it works for the training. I get different weights for each image of the task.\n- This is a problem of OVERFITTING, and is the main problem of why any of us are reaching good results. \n- This problem can be seen very well in the fact that I took the average of the weights for the training images to predict the test image, and of course, it's not working.\n\nSO,\nHow to solve overfitting?\n1. Imagine our input is an image of 10x10=100 neurons\n2. In a traditional Neural Network we can not train 100 weights with 5 examples -->  **VERY HIGH VARIANCE (OVERFITTING)** h(x)=theta0 + theta1*x1 + ... + theta100 * x100. It's very possible that we get the expected result in the training but not in the test.\n3. Possible solutions:  \n   - increase m - augment the data\n   - use regularization (increase lambda)\n   - apply PCA(Principal Component Analysis) to data compression --> less features BUT it is not recommended to avoid Overfitting. \n   - using another activation function: ReLu ?\n   \nGuys! At this point, I need your help. I'm thinking to augment the data and to apply regularization. Do you also think is the RIGHT decision?\n\nThanks a lot for your time :)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport json\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pathlib import Path\n\ndata_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\nevaluation_path = data_path / 'evaluation'\ntest_path = data_path / 'test'\n\ntraining_tasks = sorted(os.listdir(training_path))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======================================================================================\n# SHOW THE IMAGES OF 1 TASK\n# ======================================================================================\n\ncmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = colors.Normalize(vmin=0, vmax=9)\n\ndef plot_one(ax, i,train_or_test,input_or_output):\n\n    \n    input_matrix = task[train_or_test][i][input_or_output]\n    ax.imshow(input_matrix, cmap=cmap, norm=norm)\n    ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n    ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n    ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(train_or_test + ' '+input_or_output)\n\ndef plot_task(task):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"    \n    num_train = len(task['train'])\n    print('NÂº of training examples: ', num_train)\n    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n    for i in range(num_train):     \n        plot_one(axs[0,i],i,'train','input')\n        plot_one(axs[1,i],i,'train','output')        \n    plt.tight_layout()\n    plt.show()        \n        \n    num_test = len(task['test'])\n    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n    if num_test==1: \n        plot_one(axs[0],0,'test','input')\n        plot_one(axs[1],0,'test','output')     \n    else:\n        for i in range(num_test):      \n            plot_one(axs[0,i],i,'test','input')\n            plot_one(axs[1,i],i,'test','output')  \n    plt.tight_layout()\n    plt.show() \n    \nplt.figure(figsize=(5, 2), dpi=200)\nplt.imshow([list(range(10))], cmap=cmap, norm=norm)\nplt.xticks(list(range(10)))\nplt.yticks([])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======================================================================================\n# SAVE ALL THE DATA IN dict_tasks and dict_tasks_shape\n# ======================================================================================\n\n\ndict_tasks = {}\ndict_tasks_shape = {}\nfor i in range(0,len(training_tasks)): #(len(training_tasks))\n    task={}\n    task_file = str(training_path / training_tasks[i])\n    \n    with open(task_file, 'r') as f:\n        task = json.load(f)\n\n    dict_tasks[i] = task\n    dict_tasks_shape[i] = {len(task['train']),  len(task['test'])}\n    \n# Converting into list of tuple \nlist_tasks = [v for k, v in dict_tasks.items()] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======================================================================================\n# HERE WE CHANGE THE TASK WE WANT TO WORK WITH\n# ======================================================================================\n\ntask = list_tasks[11] #images for filling the constraing: 0, 2, 4, 5, 6, 9, 10, 11, 14, ...\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =========================================================================\n# SHAPE OF IMAGES -- input images and output images have usually different shapes. We want to identify them.\n# CAREFUL: inside the training images, not all of them have the same shape.\n# =========================================================================\n\n# Training images\nrows_input_images = {}\ncolumns_input_images = {}\nrows_output_images = {}\ncolumns_output_images = {}\n\nfor i in range (0,len(task['train'])):\n    # Input images\n    rows_input_images[i] = len(task['train'][i]['input'])\n    columns_input_images[i] = len(task['train'][i]['input'][0])\n    \n    #Output images\n    rows_output_images[i] = len(task['train'][i]['output'])\n    columns_output_images[i] = len(task['train'][i]['output'][0])\n    \n# Convert into LIST\nrows_input_images = [ v for k, v in rows_input_images.items()] \ncolumns_input_images = [ v for k, v in columns_input_images.items()]\nrows_output_images = [ v for k, v in rows_output_images.items()] \ncolumns_output_images = [ v for k, v in columns_output_images.items()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===========================================================================\n# WORKING WITH COLOURS IN THE NEURAL NETWORK\n# We divide the values of matrices over 10, to have all the elements of the matrices between [0,1]\n# We use the SIGMOID FUNCTION, so the output is between [0,1]\n# So here, if the output is between those values, we give them a specific value, so that then we can multiply again by 10 and plot it.\n# ==========================================================================\ndef identify_colors (output_matrix):\n    for i in range(0,rows_output_images[0]):\n        for j in range(0,columns_output_images[0]):\n            if 0.05< output_matrix[i,j] <= 0.15:\n                output_matrix[i,j] = 0.1\n            if 0.15< output_matrix[i,j] <= 0.25:\n                output_matrix[i,j] = 0.2\n            if 0.25< output_matrix[i,j] <= 0.35:\n                output_matrix[i,j] = 0.3\n            if 0.35< output_matrix[i,j] <= 0.45:\n                output_matrix[i,j] = 0.4\n            if 0.45< output_matrix[i,j] <= 0.55:\n                output_matrix[i,j] = 0.5\n            if 0.55< output_matrix[i,j] <= 0.65:\n                output_matrix[i,j] = 0.6\n            if 0.65< output_matrix[i,j] <= 0.75:\n                output_matrix[i,j] = 0.7\n            if 0.75< output_matrix[i,j] <= 0.85:\n                output_matrix[i,j] = 0.8\n            if 0.85< output_matrix[i,j] <= 0.95:\n                output_matrix[i,j] = 0.9\n            if 0.95<output_matrix[i,j]<= 1.0 or 0< output_matrix[i,j]<=0.05:\n                output_matrix[i,j] = 0.0\n                \n    return output_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======================================================================\n# WHICH MODEL TO USE? \n# ======================================================================\n\n\nclass modelNeuralNetwork () :\n    def __init__(self,rows_input_images, columns_input_images, rows_output_images, columns_output_images ):\n        \n        # We need this value to specify the shape of the NeuralNetwork of 3 layers. We want to make it proportional as we usually start with less inputs and we finish with more inputs.\n        # Example: Input--> (3x3)=9 neurons. Output --> (9x9) = 81 neurons. The hidden layers must have a number of neurons between those.\n        neurons_to_add = int((rows_output_images[0]*columns_output_images[0] - rows_input_images[0]*columns_input_images[0])/3) #we have 3 layers in our NN\n        \n        # We initialise the weights with its sizes\n        self._weights1 = np.random.uniform(low=0, high=1, size=(rows_input_images[0]*columns_input_images[0] + neurons_to_add, rows_input_images[0]*columns_input_images[0])) #(33,9)\n        self._weights2 = np.random.uniform(low=0, high=1, size=(rows_output_images[0]*columns_output_images[0] - neurons_to_add,rows_input_images[0]*columns_input_images[0] + neurons_to_add))#(57,33)\n        self._weights3 = np.random.uniform(low=0, high=1, size=(rows_output_images[0]*columns_output_images[0],rows_output_images[0]*columns_output_images[0] - neurons_to_add)) #(81,57)\n        \n        self._z1 = {}\n        self._z2 = {}\n        self._z3 = {}\n        \n        self._a0 = {}\n        self._a1 = {}\n        self._a2 = {}\n        self._a3 = {}\n                \n        self._b1 = np.zeros((rows_input_images[0]*columns_input_images[0] + neurons_to_add,1))\n        self._b2 = np.zeros((rows_output_images[0]*columns_output_images[0] - neurons_to_add,1))\n        self._b3 = np.zeros((rows_output_images[0]*columns_output_images[0],1))\n        \n        self._error3 = {}\n        self._error2 = {}\n        self._error1 = {}\n        \n        self._m = 1 #we work with 1 image at a time\n        self._alpha = 0.05\n\n    def forward_propagation(self, X_input_image):\n        self._a0 = X_input_image\n        \n        self._z1 = np.matmul(self._weights1, self._a0) + self._b1\n        self._a1 = 1 / (1+np.exp(-self._z1))\n        \n        self._z2 = np.matmul(self._weights2, self._a1) + self._b2\n        self._a2 = 1 / (1+np.exp(-self._z2))\n        \n        self._z3 = np.matmul(self._weights3, self._a2) + self._b3\n        self._a3 = 1 / (1+np.exp(-self._z3))\n        return self._a3\n        \n    \n    def backward_propagation(self, y_output_image):\n        self._error3 = self._a3 - y_output_image\n        \n        aux1 = np.matmul(np.transpose(self._weights3), self._error3)\n        aux2 = 1 / (1+np.exp(-self._z2)) * (1- 1 / (1+np.exp(-self._z2)))\n        self._error2 = np.multiply(aux1, aux2) \n        \n        aux1 = np.matmul(np.transpose(self._weights2), self._error2)\n        aux2 = 1 / (1+np.exp(-self._z1)) * (1- 1 / (1+np.exp(-self._z1)))\n        self._error1 = np.multiply(aux1, aux2) \n        \n        \n    def update_weights(self):\n        self._weights1 = self._weights1 - self._alpha/self._m* np.matmul(self._error1, np.transpose(self._a0))\n        self._weights2 = self._weights2 - self._alpha/self._m* np.matmul(self._error2, np.transpose(self._a1))\n        self._weights3 = self._weights3 - self._alpha/self._m* np.matmul(self._error3, np.transpose(self._a2))\n        \n        a = (1,1)\n        ones = np.ones(a)\n        \n        self._b1 = self._b1 - self._alpha/self._m * np.matmul(self._error1, np.transpose(ones))\n        self._b2 = self._b2 - self._alpha/self._m * np.matmul(self._error2, np.transpose(ones))\n        self._b3 = self._b3 - self._alpha/self._m * np.matmul(self._error3, np.transpose(ones))\n        \n        return self._weights1, self._weights2, self._weights3\n      \n        \n    def predict(self, X_test_image, weight1avg, weight2_avg, weight3_avg):\n        self._a0 = X_test_image\n        \n        self._z1 = np.matmul(weight1avg, self._a0) + self._b1\n        self._a1 = 1 / (1+np.exp(-self._z1))\n        \n        self._z2 = np.matmul(weight2_avg, self._a1) + self._b2\n        self._a2 = 1 / (1+np.exp(-self._z2))\n        \n        self._z3 = np.matmul(weight3_avg, self._a2) + self._b3\n        self._a3 = 1 / (1+np.exp(-self._z3))\n        self._a3 = self._a3.reshape((rows_output_images[0],columns_output_images[0]))\n        self._a3 = identify_colors(self._a3)\n        self._a3 = self._a3 * 10\n        print('Predicted Image')\n        plt.imshow(self._a3, cmap=cmap, norm=norm)\n        plt.show()\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===============================================================\n# INITIALISE THE MODEL\n# ===============================================================\nmodel = modelNeuralNetwork(rows_input_images, columns_input_images, rows_output_images, columns_output_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ==============================================================\n# THIS FUNCTION TAKES AN ARRAY AND SEE IF ALL THE ELEMENTS ARE EQUAL\n# For example, arr1 = {1,1,1,1,1}-- Output=True\n#              arr2 = {1,1,2,1,1}-- Output=False\n# ==============================================================\n\ndef checkEqualityOfDimensionsOfImages(iterator):\n   return len(set(iterator)) <= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======================================================================\n# TRAIN THE MODEL\n# ======================================================================\n\noutputs = {}\nweights = {}\n\n# If the input images have same shape. And if the ouput images have same shape\n# Input images: all of them (3x3) -->  Output images: all of them (9x9). This works fine!\nif (checkEqualityOfDimensionsOfImages(rows_input_images) == True) and (checkEqualityOfDimensionsOfImages(columns_input_images) == True) and (checkEqualityOfDimensionsOfImages(rows_output_images) == True) and (checkEqualityOfDimensionsOfImages(columns_output_images) == True):\n    print('These are the images of the training.')\n    plot_task(task)\n    print('These are the predicted images of the training.')\n    for i in range(0, len(task['train'])): #all the examples of the training images of the task\n        X_input_image = task['train'][i]['input']\n\n        X_input_image = np.asarray(X_input_image)/10\n        X_input_image = X_input_image.reshape((-1, 1)) \n    \n        y_output_image = task['train'][i]['output']\n        y_output_image = np.asarray(y_output_image)\n        y_output_image = y_output_image.reshape((-1, 1))/ 10\n    \n        for iter in range (0,50): # NUMBER OF TIMES TRAINED\n            a3 = model.forward_propagation(X_input_image)\n\n            model.backward_propagation(y_output_image)\n            weights1, weights2, weights3 = model.update_weights()\n        \n        #Store the weights and the outputs\n        weights[i] = [weights1, weights2, weights3]\n        outputs[i] = a3\n\n\n        outputs[i] = outputs[i].reshape((rows_output_images[0],columns_output_images[0]))\n        outputs[i] = identify_colors(outputs[i])\n        outputs[i] = outputs[i] * 10\n        \n       \n\n        plt.imshow(outputs[i], cmap=cmap, norm=norm)\n        plt.show()\nelse:\n    print('The shape of the images of this task are not equal.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =======================================================================\n# PREDICTION\n# =======================================================================\n\n# Load the test image. \nX_test_image = task['test'][0]['input']\n\n\n# Predict its output\nif (checkEqualityOfDimensionsOfImages(rows_input_images) == True) and (checkEqualityOfDimensionsOfImages(columns_input_images) == True) and (checkEqualityOfDimensionsOfImages(rows_output_images) == True) and (checkEqualityOfDimensionsOfImages(columns_output_images) == True):\n    print('Input image of the test data')\n    plt.imshow(X_test_image, cmap=cmap, norm=norm)\n    plt.show()\n    \n    #the weights--> i want them to be the average\n    weight1_sum = 0\n    weight2_sum = 0\n    weight3_sum = 0\n\n    for i in range (0, len(task['train'])):\n        weight1_sum += (weights[i][0])\n        weight2_sum += (weights[i][1])\n        weight3_sum += (weights[i][2])\n    \n\n    weight1_avg = weight1_sum/len(task['train'])\n    weight2_avg = weight2_sum/len(task['train'])\n    weight3_avg = weight3_sum/len(task['train'])\n\n    \n\n    X_test_image = np.asarray(X_test_image)/10\n    X_test_image = X_test_image.reshape((-1, 1))\n    model.predict(X_test_image, weight1_avg, weight2_avg, weight3_avg)\n\n\n    ## problem: it's NOT learning how the task works. It learns what pixels to colour but with a new example, those weights are not valid.\n    # overfitting \nelse:\n    print('The shape of the images of this task are not equal.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The predicted image is not equal :(\n\n# Thanks for your time. Any help is appreciated! We have to solve this! \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}