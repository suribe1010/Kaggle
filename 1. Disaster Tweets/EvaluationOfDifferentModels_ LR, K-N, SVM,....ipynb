{"cells":[{"metadata":{},"cell_type":"markdown","source":"**DIFFERENT MODELS IMPLEMENTATION FOR DETECTING DISASTER TWEETS**\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. DATA EXPLORATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\nprint(\"Data shape = \", train_data.shape)\ntrain_data.head()\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_data","execution_count":3,"outputs":[{"output_type":"stream","text":"Data shape =  (7613, 5)\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"         id keyword location  \\\n0         0     NaN      NaN   \n1         2     NaN      NaN   \n2         3     NaN      NaN   \n3         9     NaN      NaN   \n4        11     NaN      NaN   \n...     ...     ...      ...   \n3258  10861     NaN      NaN   \n3259  10865     NaN      NaN   \n3260  10868     NaN      NaN   \n3261  10874     NaN      NaN   \n3262  10875     NaN      NaN   \n\n                                                   text  \n0                    Just happened a terrible car crash  \n1     Heard about #earthquake is different cities, s...  \n2     there is a forest fire at spot pond, geese are...  \n3              Apocalypse lighting. #Spokane #wildfires  \n4         Typhoon Soudelor kills 28 in China and Taiwan  \n...                                                 ...  \n3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n3259  Storm in RI worse than last hurricane. My city...  \n3260  Green Line derailment in Chicago http://t.co/U...  \n3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n3262  #CityofCalgary has activated its Municipal Eme...  \n\n[3263 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Storm in RI worse than last hurricane. My city...</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Green Line derailment in Chicago http://t.co/U...</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#CityofCalgary has activated its Municipal Eme...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**2. DATA PREPROCESSING**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing data\ntotal = train_data.isnull().sum()\npercentage = (train_data.isnull().sum()/train_data.isnull().count()*100)\nmissing_data = pd.concat([total, percentage], axis=1, keys = ['Total', 'Percentage'])\nmissing_data","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"          Total  Percentage\nid            0    0.000000\nkeyword      61    0.801261\nlocation   2533   33.272035\ntext          0    0.000000\ntarget        0    0.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Total</th>\n      <th>Percentage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>keyword</th>\n      <td>61</td>\n      <td>0.801261</td>\n    </tr>\n    <tr>\n      <th>location</th>\n      <td>2533</td>\n      <td>33.272035</td>\n    </tr>\n    <tr>\n      <th>text</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(columns = ['id', 'location', 'keyword'])","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TEXT PREPROCESSING\n- Corpus: is a large and structured set of texts. We can consider it as simplified version of out text data that contain clean and benefit data.\n- Bag of word : In practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the text wikipedia"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\ndef clean_data (data):\n    corpus = []\n    pstem = PorterStemmer()\n\n    for i in range(data.shape[0]):\n        #Remove unwanted words\n        tweet = re.sub(\"[^a-zA-Z]\", ' ', data['text'][i])\n\n        #Lower case\n        tweet = tweet.lower()\n        tweet = tweet.split()\n    \n        #Remove stop words and steeming words(take the roots)\n        tweet = [pstem.stem(word) for word in tweet if not word in set (stopwords.words('english'))]\n        tweet = ' '.join(tweet)\n    \n        #Append clean tweet to corpus\n        corpus.append(tweet)\n    return corpus","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To reduce bag of words dimensionality, we should remove those words that are\n# repeated very few times. So, we create a dictionary where key refer to word \n# and value refer to word frequents in all tweets\n\n\n\ndef bagOfWords(data, data_corpus):\n    uniqueWordFrequents = {}\n\n    for tweet in data_corpus: \n        for word in tweet.split():\n            if(word in uniqueWordFrequents.keys()):\n                uniqueWordFrequents[word] +=1\n            else:\n                uniqueWordFrequents[word] = 1\n\n    # Convert dictionary to dataFrame\n    uniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents, orient = 'index', columns = ['Word Frequent'])\n    #uniqueWordFrequents.sort_values(by=['Word Frequent'], inplace = True, ascending=False)\n    # We take only words repeated more than 10 times\n    uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\n    \n    # Create Bag of words --> they contain only unique words in corpus\n\n    from sklearn.feature_extraction.text import CountVectorizer\n\n    counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n\n    bagWords = counVec.fit_transform(data_corpus).toarray()\n    return bagWords","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. MODELS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_corpus = clean_data(train_data)\ntrain_data_bagWords = bagOfWords(train_data, train_data_corpus)\nprint(train_data_bagWords.shape)\ntrain_data_bagWords\n","execution_count":8,"outputs":[{"output_type":"stream","text":"(7613, 787)\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data_bagWords\ny = train_data ['target']\n\nprint('X shape: ', X.shape)\nprint('y shape: ', y.shape)\nX","execution_count":9,"outputs":[{"output_type":"stream","text":"X shape:  (7613, 787)\ny shape:  (7613,)\n","name":"stdout"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=55, shuffle=True)\n\nX_train\nprint('X_train shape: ', X_train.shape)\nprint('y_train shape: ', y_train.shape)","execution_count":28,"outputs":[{"output_type":"stream","text":"X_train shape:  (6090, 787)\ny_train shape:  (6090,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 3.1. Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndecisionTreeModel = DecisionTreeClassifier (criterion = 'entropy', max_depth=None, splitter ='best', random_state = 55)\ndecisionTreeModel.fit(X_train, y_train)","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=55, splitter='best')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Error: Number of features of the model must match the input. Model n_features is 1410 and input n_features is 700 \n\n#You are supposed to pass numpy arrays and not lists as arguments to the DecisionTree, since your input was a list it gets trained as 70 features (1D list) and your test had list of 30 elements and the classifier sees it as 30 features.\n#Nonetheless, you need to reshape your input numpy array and pass it as a matrix\n#meaning: X_train.values.reshape(-1, 1) instead of X_train (it should be a numpy array not a list)\n#c.fit(X_train.values.reshape(-1, 1), y_train)\n\n#test_data_corpus = clean_data(test_data)\n\n#test_data_bagWords = bagOfWords(test_data, test_data_corpus)\n","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PROBLEMS HERE.. THE MODEL HAS BEEN TRAINED WITH A DIFFERENT NUMBER OF FEATURES\n# FROM THE TEST DATA\n# I DON'T HAVE TIME TO SOLVE IT BUT THE ERROR I THINK IS SOMEWHERE IN THE BAG OF WORDS\n\n\n#y_pred_test_data = decisionTreeModel.predict(test_data_bagWords)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_data = test_data.drop(columns=['keyword', 'location', 'text'])\n#test_data","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_data['target'] = y_pred_test_data","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nLogisticRegression = LogisticRegression(penalty='l2',solver='saga', random_state = 55)\nLogisticRegression.fit(X_train, y_train)","execution_count":34,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  \"the coef_ did not converge\", ConvergenceWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=55, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.3. Stochastic Gradient Descent Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n\nSGDClassifier = SGDClassifier(loss = 'hinge', \n                              penalty = 'l1',\n                              learning_rate = 'optimal',\n                              random_state = 55, \n                              max_iter=100)\n\nSGDClassifier.fit(X_train,y_train)","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n              max_iter=100, n_iter_no_change=5, n_jobs=None, penalty='l1',\n              power_t=0.5, random_state=55, shuffle=True, tol=0.001,\n              validation_fraction=0.1, verbose=0, warm_start=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.4. Support Vector Machine\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nSVClassifier = SVC(kernel= 'linear',\n                   degree=3,\n                   max_iter=10000,\n                   C=2, \n                   random_state = 55)\n\nSVClassifier.fit(X_train,y_train)","execution_count":36,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n  % self.max_iter, ConvergenceWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"SVC(C=2, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n    max_iter=10000, probability=False, random_state=55, shrinking=True,\n    tol=0.001, verbose=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.5. Gaussian Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngaussianNBModel = GaussianNB()\ngaussianNBModel.fit(X_train,y_train)","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"GaussianNB(priors=None, var_smoothing=1e-09)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.6. Multinomial Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmultinomialNBModel = MultinomialNB(alpha=0.1)\nmultinomialNBModel.fit(X_train,y_train)","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.7. Bernoulli Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\n\nbernoulliNBModel = BernoulliNB(alpha=0.1)\nbernoulliNBModel.fit(X_train,y_train)","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.8. K-Nearest Neighbors model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nKNeighborsModel = KNeighborsClassifier(n_neighbors = 7,\n                                       weights = 'distance',\n                                      algorithm = 'brute')\n\nKNeighborsModel.fit(X_train,y_train)","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n                     weights='distance')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.9. Gradient Boosting model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 100,\n                                                   max_depth = 30,\n                                                   random_state=55)\n\ngradientBoostingModel.fit(X_train,y_train)","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.01, loss='deviance', max_depth=30,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=55, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3.10. Vooting classifier model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nmodelsNames = [('LogisticRegression',LogisticRegression),\n               ('SGDClassifier',SGDClassifier),\n               ('SVClassifier',SVClassifier),\n               ('bernoulliNBModel',bernoulliNBModel),\n               ('multinomialNBModel',multinomialNBModel)]\n\nvotingClassifier = VotingClassifier(voting = 'hard',estimators= modelsNames)\nvotingClassifier.fit(X_train,y_train)","execution_count":42,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  \"the coef_ did not converge\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n  % self.max_iter, ConvergenceWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"VotingClassifier(estimators=[('LogisticRegression',\n                              LogisticRegression(C=1.0, class_weight=None,\n                                                 dual=False, fit_intercept=True,\n                                                 intercept_scaling=1,\n                                                 l1_ratio=None, max_iter=100,\n                                                 multi_class='auto',\n                                                 n_jobs=None, penalty='l2',\n                                                 random_state=55, solver='saga',\n                                                 tol=0.0001, verbose=0,\n                                                 warm_start=False)),\n                             ('SGDClassifier',\n                              SGDClassifier(alpha=0.0001, average=False,\n                                            class_w...\n                                  gamma='scale', kernel='linear',\n                                  max_iter=10000, probability=False,\n                                  random_state=55, shrinking=True, tol=0.001,\n                                  verbose=False)),\n                             ('bernoulliNBModel',\n                              BernoulliNB(alpha=0.1, binarize=0.0,\n                                          class_prior=None, fit_prior=True)),\n                             ('multinomialNBModel',\n                              MultinomialNB(alpha=0.1, class_prior=None,\n                                            fit_prior=True))],\n                 flatten_transform=True, n_jobs=None, voting='hard',\n                 weights=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nmodels = [decisionTreeModel, gradientBoostingModel, KNeighborsModel, LogisticRegression, \n          SGDClassifier, SVClassifier, bernoulliNBModel, gaussianNBModel, multinomialNBModel, votingClassifier]\n\nfor model in models:\n    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n    \n    y_pred = model.predict(X_test)\n    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n    print('--------------------------------------------------------------------------')","execution_count":43,"outputs":[{"output_type":"stream","text":"DecisionTreeClassifier  Train Score is   :  0.9761904761904762\nDecisionTreeClassifier  Test Score is    :  0.7419566644780039\nDecisionTreeClassifier  F1 Score is      :  0.6743993371996686\n--------------------------------------------------------------------------\nGradientBoostingClassifier  Train Score is   :  0.8586206896551725\nGradientBoostingClassifier  Test Score is    :  0.7544320420223244\nGradientBoostingClassifier  F1 Score is      :  0.6375968992248061\n--------------------------------------------------------------------------\nKNeighborsClassifier  Train Score is   :  0.9761904761904762\nKNeighborsClassifier  Test Score is    :  0.7406434668417596\nKNeighborsClassifier  F1 Score is      :  0.5872518286311389\n--------------------------------------------------------------------------\nLogisticRegression  Train Score is   :  0.8502463054187193\nLogisticRegression  Test Score is    :  0.7826657912015759\nLogisticRegression  F1 Score is      :  0.7230125523012553\n--------------------------------------------------------------------------\nSGDClassifier  Train Score is   :  0.8600985221674877\nSGDClassifier  Test Score is    :  0.768220617202889\nSGDClassifier  F1 Score is      :  0.7132412672623883\n--------------------------------------------------------------------------\nSVC  Train Score is   :  0.8574712643678161\nSVC  Test Score is    :  0.7741300065659882\nSVC  F1 Score is      :  0.7180327868852457\n--------------------------------------------------------------------------\nBernoulliNB  Train Score is   :  0.8091954022988506\nBernoulliNB  Test Score is    :  0.7774130006565988\nBernoulliNB  F1 Score is      :  0.7129551227773073\n--------------------------------------------------------------------------\nGaussianNB  Train Score is   :  0.7893267651888342\nGaussianNB  Test Score is    :  0.7669074195666448\nGaussianNB  F1 Score is      :  0.6728110599078342\n--------------------------------------------------------------------------\nMultinomialNB  Train Score is   :  0.8022988505747126\nMultinomialNB  Test Score is    :  0.7734734077478661\nMultinomialNB  F1 Score is      :  0.7165160230073953\n--------------------------------------------------------------------------\nVotingClassifier  Train Score is   :  0.8512315270935961\nVotingClassifier  Test Score is    :  0.7852921864740644\nVotingClassifier  F1 Score is      :  0.7263598326359832\n--------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}