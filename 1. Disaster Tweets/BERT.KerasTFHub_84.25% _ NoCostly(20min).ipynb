{"cells":[{"metadata":{},"cell_type":"markdown","source":"This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow/models/official/nlp/bert. It uses L=24 hidden layers (i.e., Transformer blocks), a hidden size of H=1024, and A=16 attention heads.\n\nThis model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n\nAll parameters in the module are trainable, and fine-tuning all parameters is the recommended practice."},{"metadata":{},"cell_type":"markdown","source":"- No pooling, directly use the CLS embedding.\n- No dense layer. Simply add a sigmoid output directly to the last layer of BERT, not to the intermediate layers.\n- Fixed learning rate, batch size, epochs, optimizer. Adam optimizer is used. Learning rate: 2e-5 and 5e-5. Epochs=3. Batch-size=32. These values are used in the original paper.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization #the above script","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. FUNCTIONS WE WILL USE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TOKENIZATION\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text  in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len - 2] #i think it is because we are gonna add [CLS] and [SEP]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"] #The first token of every sequence is always a special classification token ([CLS]).\n                                                      #It is a special symbol added in front of every input example and [SEP] is a special\n                                                      #separator token is added at the end of every input example.\n        print(\"Length of input sequence: \", len(input_sequence))\n        \n        pad_len = max_len - len(input_sequence)\n        print(\"Length of padding: \", pad_len)\n        \n        #Token ids from Tokenizer vocab\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) #input token ids is tokenizer converts tokens using vocab file.\n        tokens += [0] * pad_len \n        print(\"Tokens: \", tokens)\n        \n        pad_masks = [1] * len(input_sequence) + [0] * pad_len #input masks are either 0 or 1. 1 for useful tokens, 0 for padding.\n    print(\"Input_mask: \", input_mask)\n        print(\"Padding_masks: \", pad_masks)\n        \n        segment_ids = [0] * max_len  #here everyone is 0\n        #segment ids are either 0 or 1. For 2 text training: 0 for the first one, 1 for the second one.\n        print(\"Ids of segments: \", segment_ids)\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    \n    #PREPARE THE MODEL\n    input_word_ids = Input(shape = (max_len), dtype = tf.int32, name = \"input_words_ids\")\n    print(\"Input_word_ids: \", input_word_ids)\n    input_mask = Input(shape = (max_len), dtype = tf.int32, name = \"input_mask\") \n    segment_ids = Input(shape = (max_len), dtype = tf.int32, name = \"segment_ids\")\n    print(\"Segment_ids: \", segment_ids)\n    \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])# pooled_output representations the entire input sequences and sequence_output representations each input token in the context.\n    print(\"Sequence_output: \", sequence_output)\n    \n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation = 'sigmoid')(clf_output) #A simple classification layer is added to the pre-trained model, \n                                                       #and all parameters are jointly fine-tuned on a downstream task.\n    print(\"Added layer 'out': \", out)\n    \n    model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out)\n    model.compile(Adam(lr=2e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA**\n- Load BERT from Tensorflow Hub\n- Load tokenizer from the bert layer\n- Encode the text into tokens, masks, and segment flags"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain_data = train_data.head(100)\n\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_data = test_data.head(100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#During any text data preprocessing, there is a tokenization phase involved. \n#The tokenizer available with the BERT package is very powerful.\n#We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. \n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nvocab_file\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ndo_lower_case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the tokenizer with the BERT layer and import it tokenizer using the original vocab file.\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\ntokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train_data.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test_data.text.values, tokenizer, max_len=160)\ntrain_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_data.target.values\ntrain_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = model.fit(train_input, train_labels, validation_split = 0.2, epochs = 1, batch_size = 1)\ntrain_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['target'] = test_pred.round().astype(int)\ntest_data = test_data.drop(columns = ['keyword', 'location', 'text'])\ntest_data.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CONCLUSION ACCURACY\n- Less epochs and batch_size gives better accuracy.\n- Higher validation than 0.2 set gives worse results. The opposite also gives worst results.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}