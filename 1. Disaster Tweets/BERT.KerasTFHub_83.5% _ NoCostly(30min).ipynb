{"cells":[{"metadata":{},"cell_type":"markdown","source":"This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow/models/official/nlp/bert. It uses L=24 hidden layers (i.e., Transformer blocks), a hidden size of H=1024, and A=16 attention heads.\n\nThis model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n\nAll parameters in the module are trainable, and fine-tuning all parameters is the recommended practice."},{"metadata":{},"cell_type":"markdown","source":"- No pooling, directly use the CLS embedding.\n- No dense layer. Simply add a sigmoid output directly to the last layer of BERT, not to the intermediate layers.\n- Fixed learning rate, batch size, epochs, optimizer. Adam optimizer is used. Learning rate: 2e-5 and 5e-5. Epochs=3. Batch-size=32. These values are used in the original paper.\n\n"},{"metadata":{},"cell_type":"markdown","source":"ReferencesÂ¶\n\n   - Source for bert_encode function: https://www.kaggle.com/user123454321/bert-starter-inference\n   - All pre-trained BERT models from Tensorflow Hub: https://tfhub.dev/s?q=bert\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization #the above script","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. FUNCTIONS WE WILL USE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text  in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len - 2] #i think it is because we are gonna add [CLS] and [SEP]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        print(\"Length of input sequence: \", len(input_sequence))\n        \n        pad_len = max_len - len(input_sequence)\n        print(\"Length of padding: \", pad_len)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len \n        print(\"Tokens: \", tokens)\n        \n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        print(\"Padding_masks: \", pad_masks)\n        \n        segment_ids = [0] * max_len\n        print(\"Ids of segments: \", segment_ids)\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape = (max_len), dtype = tf.int32, name = \"input_words_ids\")\n    input_mask = Input(shape = (max_len), dtype = tf.int32, name = \"input_mask\")\n    segment_ids = Input(shape = (max_len), dtype = tf.int32, name = \"segment_ids\")\n    \n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation = 'sigmoid')(clf_output)\n    \n    model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out)\n    model.compile(Adam(lr=2e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA**\n- Load BERT from Tensorflow Hub\n- Load tokenizer from the bert layer\n- Encode the text into tokens, masks, and segment flags"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nsubmission_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train_data.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test_data.text.values, tokenizer, max_len=160)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_data.target.values\ntrain_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = model.fit(train_input, train_labels, validation_split = 0.2, epochs = 3, batch_size = 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['target'] = test_pred.round().astype(int)\ntest_data = test_data.drop(columns = ['keyword', 'location', 'text'])\ntest_data.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}